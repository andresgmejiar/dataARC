{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: gdata\n",
      "\n",
      "Warning message:\n",
      "\"package 'gdata' was built under R version 3.6.3\"\n",
      "gdata: Unable to locate valid perl interpreter\n",
      "gdata: \n",
      "gdata: read.xls() will be unable to read Excel XLS and XLSX files\n",
      "gdata: unless the 'perl=' argument is used to specify the location of a\n",
      "gdata: valid perl intrpreter.\n",
      "gdata: \n",
      "gdata: (To avoid display of this message in the future, please ensure\n",
      "gdata: perl is installed and available on the executable search path.)\n",
      "\n",
      "gdata: Unable to load perl libaries needed by read.xls()\n",
      "gdata: to support 'XLX' (Excel 97-2004) files.\n",
      "\n",
      "\n",
      "\n",
      "gdata: Unable to load perl libaries needed by read.xls()\n",
      "gdata: to support 'XLSX' (Excel 2007+) files.\n",
      "\n",
      "\n",
      "\n",
      "gdata: Run the function 'installXLSXsupport()'\n",
      "gdata: to automatically download and install the perl\n",
      "gdata: libaries needed to support Excel XLS and XLSX formats.\n",
      "\n",
      "\n",
      "Attaching package: 'gdata'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    nobs\n",
      "\n",
      "\n",
      "The following object is masked from 'package:utils':\n",
      "\n",
      "    object.size\n",
      "\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    startsWith\n",
      "\n",
      "\n",
      "Loading required package: R.utils\n",
      "\n",
      "Warning message:\n",
      "\"package 'R.utils' was built under R version 3.6.3\"\n",
      "Loading required package: R.oo\n",
      "\n",
      "Warning message:\n",
      "\"package 'R.oo' was built under R version 3.6.3\"\n",
      "Loading required package: R.methodsS3\n",
      "\n",
      "Warning message:\n",
      "\"package 'R.methodsS3' was built under R version 3.6.3\"\n",
      "R.methodsS3 v1.8.1 (2020-08-26 16:20:06 UTC) successfully loaded. See ?R.methodsS3 for help.\n",
      "\n",
      "R.oo v1.24.0 (2020-08-26 16:11:58 UTC) successfully loaded. See ?R.oo for help.\n",
      "\n",
      "\n",
      "Attaching package: 'R.oo'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:R.methodsS3':\n",
      "\n",
      "    throw\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:gdata':\n",
      "\n",
      "    ll, trim\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:methods':\n",
      "\n",
      "    getClasses, getMethods\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    attach, detach, load, save\n",
      "\n",
      "\n",
      "R.utils v2.10.1 (2020-08-26 22:50:31 UTC) successfully loaded. See ?R.utils for help.\n",
      "\n",
      "\n",
      "Attaching package: 'R.utils'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:gdata':\n",
      "\n",
      "    env, resample\n",
      "\n",
      "\n",
      "The following object is masked from 'package:utils':\n",
      "\n",
      "    timestamp\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    cat, commandArgs, getOption, inherits, isOpen, nullfile, parse,\n",
      "    warnings\n",
      "\n",
      "\n",
      "Loading required package: rgdal\n",
      "\n",
      "Warning message:\n",
      "\"package 'rgdal' was built under R version 3.6.3\"\n",
      "Loading required package: sp\n",
      "\n",
      "Warning message:\n",
      "\"package 'sp' was built under R version 3.6.3\"\n",
      "rgdal: version: 1.5-18, (SVN revision 1082)\n",
      "Geospatial Data Abstraction Library extensions to R successfully loaded\n",
      "Loaded GDAL runtime: GDAL 3.0.4, released 2020/01/28\n",
      "Path to GDAL shared files: C:/Users/hp/Documents/R/win-library/3.6/rgdal/gdal\n",
      "GDAL binary built with GEOS: TRUE \n",
      "Loaded PROJ runtime: Rel. 6.3.1, February 10th, 2020, [PJ_VERSION: 631]\n",
      "Path to PROJ shared files: C:\\ProgramData\\Anaconda3\\Library\\share\n",
      "Linking to sp version:1.4-4\n",
      "To mute warnings of possible GDAL/OSR exportToProj4() degradation,\n",
      "use options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading rgdal.\n",
      "Overwritten PROJ_LIB was C:\\ProgramData\\Anaconda3\\Library\\share\n",
      "\n",
      "\n",
      "Attaching package: 'rgdal'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:R.oo':\n",
      "\n",
      "    getDescription\n",
      "\n",
      "\n",
      "Loading required package: raster\n",
      "\n",
      "Warning message:\n",
      "\"package 'raster' was built under R version 3.6.3\"\n",
      "\n",
      "Attaching package: 'raster'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:R.utils':\n",
      "\n",
      "    extract, resample\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:R.oo':\n",
      "\n",
      "    extend, trim\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:gdata':\n",
      "\n",
      "    resample, trim\n",
      "\n",
      "\n",
      "Loading required package: rbioclim\n",
      "\n",
      "Warning message:\n",
      "\"package 'rbioclim' was built under R version 3.6.3\"\n",
      "\n",
      "Attaching package: 'rbioclim'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:raster':\n",
      "\n",
      "    getData\n",
      "\n",
      "\n",
      "Loading required package: stringr\n",
      "\n",
      "Warning message:\n",
      "\"package 'stringr' was built under R version 3.6.3\"\n",
      "Loading required package: stringi\n",
      "\n",
      "Warning message:\n",
      "\"package 'stringi' was built under R version 3.6.3\"\n",
      "Loading required package: proj4\n",
      "\n",
      "Warning message:\n",
      "\"package 'proj4' was built under R version 3.6.3\"\n",
      "\n",
      "Attaching package: 'proj4'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:rgdal':\n",
      "\n",
      "    project\n",
      "\n",
      "\n",
      "Loading required package: data.table\n",
      "\n",
      "Warning message:\n",
      "\"package 'data.table' was built under R version 3.6.3\"\n",
      "\n",
      "Attaching package: 'data.table'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:raster':\n",
      "\n",
      "    shift\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:gdata':\n",
      "\n",
      "    first, last\n",
      "\n",
      "\n",
      "Loading required package: FSA\n",
      "\n",
      "Warning message:\n",
      "\"package 'FSA' was built under R version 3.6.3\"\n",
      "## FSA v0.8.32. See citation('FSA') if used in publication.\n",
      "## Run fishR() for related website and fishR('IFAR') for related book.\n",
      "\n",
      "Loading required package: plyr\n",
      "\n",
      "Warning message:\n",
      "\"package 'plyr' was built under R version 3.6.3\"\n",
      "Loading required package: wordcloud\n",
      "\n",
      "Warning message:\n",
      "\"package 'wordcloud' was built under R version 3.6.3\"\n",
      "Loading required package: RColorBrewer\n",
      "\n",
      "Loading required package: KernSmooth\n",
      "\n",
      "Warning message:\n",
      "\"package 'KernSmooth' was built under R version 3.6.3\"\n",
      "KernSmooth 2.23 loaded\n",
      "Copyright M. P. Wand 1997-2009\n",
      "\n",
      "Loading required package: gdtools\n",
      "\n",
      "Warning message:\n",
      "\"package 'gdtools' was built under R version 3.6.3\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. TRUE\n",
       "2. TRUE\n",
       "3. TRUE\n",
       "4. TRUE\n",
       "5. TRUE\n",
       "6. TRUE\n",
       "7. TRUE\n",
       "8. TRUE\n",
       "9. TRUE\n",
       "10. TRUE\n",
       "11. TRUE\n",
       "12. TRUE\n",
       "13. TRUE\n",
       "14. TRUE\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] TRUE\n",
       "\n",
       "[[2]]\n",
       "[1] TRUE\n",
       "\n",
       "[[3]]\n",
       "[1] TRUE\n",
       "\n",
       "[[4]]\n",
       "[1] TRUE\n",
       "\n",
       "[[5]]\n",
       "[1] TRUE\n",
       "\n",
       "[[6]]\n",
       "[1] TRUE\n",
       "\n",
       "[[7]]\n",
       "[1] TRUE\n",
       "\n",
       "[[8]]\n",
       "[1] TRUE\n",
       "\n",
       "[[9]]\n",
       "[1] TRUE\n",
       "\n",
       "[[10]]\n",
       "[1] TRUE\n",
       "\n",
       "[[11]]\n",
       "[1] TRUE\n",
       "\n",
       "[[12]]\n",
       "[1] TRUE\n",
       "\n",
       "[[13]]\n",
       "[1] TRUE\n",
       "\n",
       "[[14]]\n",
       "[1] TRUE\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in get(genname, envir = envir) : object 'testthat_print' not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping install of 'rbioclim' from a github remote, the SHA1 (662b653b) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list.of.packages <- c(\"gdata\",\"R.utils\",\"rgdal\",\"raster\",\"rbioclim\",\"stringr\",\"stringi\",\n",
    "                      \"proj4\",\"data.table\",\"FSA\",\"plyr\",\"wordcloud\",\"KernSmooth\",\"gdtools\")\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages) > 0) install.packages(new.packages)\n",
    "\n",
    "lapply(list.of.packages, FUN = function(X) {\n",
    "    do.call(\"require\", list(X)) \n",
    "})\n",
    "\n",
    "devtools::install_github(\"MoisesExpositoAlonso/rbioclim\") # To download WorldClim\n",
    "library(rbioclim) # To download WorldClim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory Setup and Data Download\n",
    "\n",
    "As part of this exercise, we'll have to download climate and topographic information. The default directory to which files will be downloaded is the current working directory. If you'd like to specify a different folder, change \"getwd()\" to the filepath with frontslashes between quotes. (e.g. \"I:/DataArc/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path <- getwd()\n",
    "setwd(path)\n",
    "set.seed(1601720)\n",
    "rasterOptions(maxmemory=1.1e9)\n",
    "\n",
    "# Set up necessary directories\n",
    "if (!dir.exists(\"WorldClim\")) {dir.create(\"WorldClim\")}\n",
    "if (!dir.exists(\"DEM\")) {dir.create(\"DEM\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also load the \"Translator.R\" file, that contains a custom function designed to convert an output JSON from the dataARC API to a parsable data.table or CSV. To use it, simply call \"arc.dataconvert()\" on a JSON or compressed JSON (.json.gz). If you'd like to export the converted table, simply add the argument \"export = filepath\" where filepath is the desired output file in .csv or .gz formats. Users who desire greater control over the data structure can modify the \"Translator.R\" file directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(filename, \"r\", encoding = encoding):\n",
      "\"cannot open file 'Translator.R': No such file or directory\"\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(filename, \"r\", encoding = encoding): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in file(filename, \"r\", encoding = encoding): cannot open the connection\nTraceback:\n",
      "1. source(\"Translator.R\")",
      "2. file(filename, \"r\", encoding = encoding)"
     ]
    }
   ],
   "source": [
    "source(\"Translator.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download the necessary rasters from the internet. This particular getData function from the rbioclim library first checks to see if the files already exist locally in the required directory and if not downloads them from the UC Davis repository. The provided resolutions are 2.5 arcminutes (around 1957 m at this latitude). They will be downsampled in a later step.\n",
    "\n",
    "We start with the modern climate rasters. For time's sake  we are only downloading the pre-preprocessed bioclimatic variables for annual values although the user is free to change the download to \"tmin\", \"tmax\", or \"prec\" for monthly values of raw variables. Unfortunately, rbioclim does not provide a way to only extract a single variable, so this could take a while (~10 min depending on the computer for both sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_p <- recursive.getData(\"pres\",path=\"WorldClim\",var=\"bio\")\n",
    "clim_p <- clim_p$pres[[1]]\n",
    "plot(clim_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're only selecting the first raster of the downloaded rasterBrick (the [[1]] subset). This means that the preprocessing will only be appled to the mean annual temperature raster. If the user would like to work with different types of climactic variables, they may change the subset according to the indices listed at: https://www.worldclim.org/data/bioclim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next come the climate predictions. WorldClim provides predictions based on the CMIP6 model (CC prefix), which has predictions for 2050 and 2070 (final two digits of download code) at using four different climate scenarios (representative concentration pathways): 2.6, 4.5, 6.0, and 8.5 (first two digits of download code). 8.5 is generally taken as the worst-case scenario, but it is increasingly seen as unrepresentative of the most likely case given trends in CO2 emissions (https://www.nature.com/articles/d41586-020-00177-3). \n",
    "\n",
    "In this example we use 2.6--one of the best-case scenarios-- due to its proximity to the pathway aspired to by the Paris agreements, although the user is free to change both the year and pathway. For example, if we wanted to get the 8.5 pathway prediciton for 2070, we'd download CC8570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_f <- recursive.getData(\"CC2650\",path=\"WorldClim\",var=\"bio\")\n",
    "clim_f <- clim_f$CC2650[[1]]\n",
    "plot(clim_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's fine if you get a bunch (19) of Warnings. We'll reproject the data so it won't be an issue. \n",
    "\n",
    "Finally, the DEM is a little tricker. Iceland lies outside of the range covered  by the SRTM STS-99 mission and does not have publically-available national lidar coverage. The resource used in this demonstration is obtained from digitized topographic maps from declassified Russian intelligence sources. Data are provided at 15 arcsecond resolution (around 196 m at this latitude) but also available are 1\", 3\" and 5\" from the website.\n",
    "\n",
    "Due to computational limits at 2GB of memory for binders, we provide a pre-upsampled DEM. If this file is present in the main directory, the program will load that as the DEM. If not, it will check to see if the raw Russian data has been downloaded, and if so use that. If it hasn't, download and uncompress the necessary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (file.exists(\"Iceland_DEM.tif\")) {\n",
    "    dem <- raster(\"Iceland_DEM.tif\")\n",
    "} else {\n",
    "    if (!file.exists(\"DEM/15-C.tif\")) {\n",
    "  download.file(\"http://www.viewfinderpanoramas.org/DEM/TIF15/15-C.zip\", \n",
    "                destfile=\"DEM/isl_2010.zip\")\n",
    "  unzip(\"DEM/isl_2010.zip\",exdir=\"DEM\")\n",
    "        }\n",
    "dem <- raster(\"DEM/15-C.tif\",sep=\"\")\n",
    "    }\n",
    "plot(dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you don't want to keep the originally-downloaded WorldClim and DEM files, you can delete them running the cell below. If you are running this script locally, considering using them again and are not short of storage space, we recommend keeping them to drastically reduce the processing time in the future.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeRaster(dem,\"Iceland_DEM.tif\",type=\"GTiff\",overwrite=TRUE)\n",
    "writeRaster(clim_f,\"Iceland_Clim_Future.tif\",type=\"GTiff\",overwrite=TRUE)\n",
    "writeRaster(clim_p,\"Iceland_Clim_Present.tif\",type=\"GTiff\",overwrite=TRUE)\n",
    "\n",
    "dem <- raster(\"Iceland_DEM.tif\")\n",
    "clim_f <- raster(\"Iceland_Clim_Future.tif\")\n",
    "clim_p <- raster(\"Iceland_Clim_Present.tif\")\n",
    "\n",
    "unlink(\"topo\",recursive=TRUE)\n",
    "unlink(\"WorldClim\",recursive=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raster Preprocessing\n",
    "\n",
    "Now that we have downloaded the necessary files, we need to prepare them for analysis. For this, we need to address three issues:\n",
    "1. The rasters are unprojected, with XY coorinates (planar variables), actually standing  for latitude/longitude (altitude and azimuth in spherical)\n",
    "2. The rasters have different extents, all larger than the area of interest (Iceland),\n",
    "3. The rasters are of different resolutions and have different origins. \n",
    "\n",
    "First, we'll define the projection we want and a bounding box that will cover the area of interest. Projections transform spherical/geodesic data (latitude and longitude relative to the WGS1984 spheroid) to planar data (XY coordinates on a flat surface). Since we're interested in performing operations related to area (such as densities) or distance, we need a conformal projection that minimizes both types of error. \n",
    "\n",
    "Iceland uses EPSG:9040---a Lambert Conformal Conic projection defined with the ISL2016 datum, but for most practical purposes WGS1984 will do. The _init_ file for proj4 included in this binder release does not have access to the complete list of EPSG codes, and as such the simple EPSG import has been commented out in this script. Users may employ that line if they are running the notebook or associated scripts locally. The projections, however, are identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proj <- CRS(\"+init=epsg:9040 +datum=WGS84\")\n",
    "proj <- CRS(\"+proj=lcc +lat_0=52 +lon_0=10 +lat_1=35 +lat_2=65 +x_0=4000000 +y_0=2800000\n",
    "                    +datum=WGS84 +units=m +no_defs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to crop the rasters to make them more manageble. Ideally, this should be no larger than our area of interest since the time needed toy process a raster increases according to the product of its length times its witdh---so large rasters get ver slow very quickly. The smaller the better. \n",
    "\n",
    "We'll define a blank raster that describes the area we're interested in in the target projection, then we'll transform its extent to the original projections of thedownloaded data, and finally we'll crop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext <- raster()  # Dummy target extent raster\n",
    "extent(ext) <- extent(2400000,3000000,4300000,4700000)\n",
    "crs(ext) <- proj\n",
    "\n",
    "ext_proj <- projectExtent(ext,dem) # The DEM\n",
    "dem <- crop(dem,ext_proj)\n",
    "dem[dem <= 0] <- NA # Since the ocean is reported as having Z = 0---we want it as NA\n",
    "\n",
    "ext_proj <- projectExtent(ext,clim_f) # The WorldClim data. \n",
    "clim_f <- crop(clim_f,ext_proj)\n",
    "clim_p <- crop(clim_p,ext_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the rasters are much smaller, we can reproject them to the target projection. We set the target resolution of the DEM at 200 m  since it's a friendlier number and bilinear interpolation is performed during the resampling, so we're drawing elevation estimates from an otherwise smooth model. We'll crop the model one more time  with an unprojected target extent to make sure that all of our  final analysis rasters are exactly the same size and have the same origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem <- projectRaster(dem,crs=crs(ext),res=200)\n",
    "dem <- crop(dem,ext)\n",
    "plot(dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the climate rasters, we want to downsample them so that the final products are of the same (smaller/finer) resolution as  the DEM. So we'll project them to the closest integer resolution in meters,  resample them to match the DEM's resolution and origin, and crop them with an unprojected extent. \n",
    "\n",
    "***NOTE THAT RESAMPLING DOES NOT GENERATE NEW INFORMATION*** \n",
    "\n",
    "You cannot get \"more accurate\" estimates for a particular location by downsampling them. Rather, you assume that the general trend at the coarser resolution is consistent over each step/pixel, and  estimate that tendency at a particular point. It cannot account for variances at smaller resolutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_f <- projectRaster(clim_f,crs=crs(ext),res=1957) #Project\n",
    "clim_p <- projectRaster(clim_p,crs=crs(ext),res=1957)\n",
    "\n",
    "clim_f <- resample(clim_f,dem) # Resample\n",
    "clim_p <- resample(clim_p,dem)\n",
    "\n",
    "clim_f <- crop(clim_f,ext) # and Crop\n",
    "clim_p <- crop(clim_p,ext)\n",
    "\n",
    "plot(clim_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can finally move on to the analyses, we should (optionally) clean up our directories of files we won't use anymore and save the final data rasters so we don't have to re-generate them each time.\n",
    "\n",
    "Export your rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeRaster(dem,\"Iceland_DEM.tif\",type=\"GTiff\",overwrite=TRUE)\n",
    "writeRaster(clim_f,\"Iceland_Clim_Future.tif\",type=\"GTiff\",overwrite=TRUE)\n",
    "writeRaster(clim_p,\"Iceland_Clim_Present.tif\",type=\"GTiff\",overwrite=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you've already generated these files and for some reason need to restart your session, you just need to import those rasters using the raster() function (use stack() for the climate raster if you used more than one subset bioclim variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataARC Preprocessing\n",
    "\n",
    "Now we can finally import the DataARC output. You can use R's base class of data.frame, but it can be very slow for export operations, and while indexing large datasets such as DataARC. Instead, we'll use data.table for most of the operations.   We'll also \"lapply\" the *trimws* function to remove any whitespace to make things parsable. More on \"lapply\" later.  Starting from here, you need to make sure that the following files are in the current working directory:\n",
    "\n",
    "1. OUT_WITH_CONCEPTS_COMBINATORS_RELATED.csv, containing the dataARC observations\n",
    "2. CONCEPT_DATA_FRAME.csv, containing the concept hashes\n",
    "3. DATASET_HASH_OUT.csv, containing the hashes for each original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arc <- arc.dataconvert(\"results.json.gz\")\n",
    "setnames(arc,names(arc),c(\"NUMBER\",\"ID\",\"Y\",\"X\",\"DATASET\",\"CATEGORY\",\"ENTRY\",\"CONCEPT\",\"COMBINATOR\",\"RELATED\",\"CONTEXT\"))\n",
    "arc[, names(arc) := lapply(.SD,trimws,which='left')]\n",
    "\n",
    "tail(arc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about data.tables is that we can manipulate the data in-place without having to generate a copy first. So, for example if there are entries in the XY columns that don't contain numbers, the columns are imported as if they were text. We'll use as.numeric to ensure that they're numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc[,c(\"X\",\"Y\") := lapply(.SD,as.numeric), .SDcols = c(\"X\",\"Y\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data.tables are like data.frames, but far more powerful. Most of the action occurs in the indexing parameters, in the form [i,j,k,SD] where \"i\" is a number or a logical condition to filter rows, \"j\" how columns will be manipulated and returned,  \"k\" the features by which we can group, and SD indicating which columns to use for operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used the ':=' symbol and didn't assign the output to a new variable or re-assign it to the original arc variable. The := symbol changes it in-place. .SD tells it to iteratively apply that function to all columns defined under .SDcols. We could have created *new* columns by simply changing the name of the strings in the first vector in the j slot.\n",
    "\n",
    "DataARC also reports coordinates in unprojected decimal degrees according to the WGS1984 spheroid. We need to transform these values to the appropriate projection. \n",
    "\n",
    "The function we use to project raw coordinates in decimal degrees form expects radians in unprojected systems, so we first need to convert the XY to radians according to 180° = π rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc[, `:=`(X = X * ..pi / 180, Y = Y * ..pi / 180)]\n",
    "\n",
    "# Now we can convert to our projection system\n",
    "arc[,`:=`(X = ptransform(.(X,Y),src.proj=CRS(\"+proj=longlat +datum=WGS84\"),\n",
    "                         dst.proj=proj)[[1]],\n",
    "          Y = ptransform(.(X,Y),src.proj=CRS(\"+proj=longlat +datum=WGS84\"),\n",
    "                         dst.proj=proj)[[2]])]\n",
    "arc[1:25,.(ID,X,Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting dataARC Observations\n",
    "\n",
    "Let's start off by making a plot of all the sites in the datasets. For this, we'll group by ID (since it belongs to different observations for each individual site) in the 3rd/k column, and take the mean of each  and X and Y column.  In theory, the X and Y should be the same for each entry with the same ID, so we could also use min() or max(), or list(unique()). We'll use the .SD feature so that we can use lapply to iteratively get the mean on each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output <- arc[,lapply(.SD,mean), by=ID, .SDcols = c(\"X\",\"Y\")][,.(X,Y)]\n",
    "output <- na.omit(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert this from a data.table to a SpatialPointsDataFrame object (a vector format, akin to a shapefile with point values), and plot onto the DEM using the raster package's plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output <- SpatialPointsDataFrame(output[,.(X,Y)],output,proj4string=proj)\n",
    "plot(dem)\n",
    "plot(output,add=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the *mapview* library and view it on an interactive map. This will let us see what the entries are by clicking on each poins. If you're reading this as a Jupyter notebook, make sure that [\"ipywidgets\"](https://ipywidgets.readthedocs.io/en/latest/user_install.html), [\"ipyleaflet\"](https://ipyleaflet.readthedocs.io/en/latest/installation.html), and [\"r-essentials\"](https://anaconda.org/r/r-essentials)  have been installed with pip or conda. This example will not work in a binder, and has therefore been commented out. Otherwise, the R console or RStudio output windows can handle *mapview* without the need to go into the command line. \n",
    "\n",
    "In this example we plot with both in different cells in the event the user does not wish to install ipywidgets, ipyleaflet, and r-essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapview::mapview(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *mapview* plot is useful, but all of the concepts have been replaced by their hash. \n",
    "To tie a hash to a concept, we need to bring in the concepts dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts <- read.csv(\"CONCEPT_DATA_FRAME.csv\",stringsAsFactors = FALSE)\n",
    "concepts <- as.data.table(concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot all of the sites tied to the concept \"butchery\".\n",
    "First we get the correct hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query <- concepts[concepts$NAME == \"butchery\",HASH] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we filter DataArc rows according to those that have the butchery hash substring in the concept strings. The output file reports lists of concepts as strings of individual hashes separated by semicolons, which is why we use the str_detect function and not the %in% operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output <- arc[str_detect(CONCEPT,query), # Filters by hash, gets XY\n",
    "              lapply(.SD,mean), by=.(ID,CONCEPT), .SDcols = c(\"X\",\"Y\")][,.(X,Y,CONCEPT)]\n",
    "output <- na.omit(output) # Drops missing values\n",
    "output <- SpatialPoints(output[,.(X,Y)],proj4string=proj) # Adds projection information\n",
    "#mapview::mapview(output, map.types = \"OpenTopoMap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(dem)\n",
    "plot(output, add=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about something a little bit more difficult. \n",
    "For a complex example with multiple entries, let's search for concepts that may be related to farms or agriculture,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query <- c(\"agricultur| farm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of info about the specific query. \n",
    "- The \"|\" tells stringr that there are multiple candidate strings to look for, namely \"agricult\" and \" farm\"\n",
    "- We searched for \"agricultur\" so that we can find anything containing \"agriculture\" and \"agricultural\"\n",
    "- We added a space before \"farm\" (i.e. \" farm\") to explicitly reject entries  that are ***only*** farm. This will return concepts where farm is not the first word. We do this to limit the number of sites that are displayed, otherwise the display will be saturated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash <- concepts[str_detect(concepts$NAME, query)] # Finds the right hashes\n",
    "hashes <- paste(hash$HASH,collapse=\"|\") # Puts them in a format readable by stringr\n",
    "output <- arc[str_detect(CONCEPT,hashes), # Filters by hash, gets XY and concept\n",
    "              lapply(.SD,mean), by=.(ID,CONCEPT), .SDcols = c(\"X\",\"Y\")][,.(X,Y,CONCEPT)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the butchery example, we know we have different concepts that may or may not be found in the same locations. So we need to group not only by XY but also by *individual* concepts to properly plot the data. This means that we need to convert the concepts string to a list using tstrsplit, and then bust each row so that each element in the concept list  gets its own row using unlist. We now can then group by XY. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output <- output[, .(CONCEPT = unlist(tstrsplit(CONCEPT,\":\"))), by=c(\"X\",\"Y\")] # Convert to List\n",
    "output <- output[CONCEPT %in% hash$HASH,.(CONCEPT),by=c(\"X\",\"Y\")] # Filter unwanted concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting with the base/raster \"plot\" function requires that we give each concept a dummy index so that we can plot it with different symbols. For mapview we just passs the \"CONCEPT\" column to zcols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[, `:=`(DUMMY = match(CONCEPT,..hash$HASH))]\n",
    "output <- na.omit(output) # Drops missing values\n",
    "output <- SpatialPointsDataFrame(output[,.(X,Y)],output[,.(CONCEPT,DUMMY)],\n",
    "                                 proj4string=proj) # Adds projection information\n",
    "output$CONCEPT <- hash$NAME[match(output$CONCEPT, hash$HASH)]\n",
    "\n",
    "# with MapView\n",
    "#mapview::mapview(output, zcols = \"CONCEPT\", map.types = \"OpenTopoMap\") # Plot the result\n",
    "\n",
    "# with plot\n",
    "# Plot the DEM again to clear the display, then plot the points\n",
    "plot(dem)\n",
    "plot(output, pch=(1:length(output))[output@data$DUMMY],\n",
    "     col=(1:length(output))[output@data$DUMMY],\n",
    "     add= TRUE)\n",
    "\n",
    "# Add a legend\n",
    "legend(\"topright\",hash$NAME,\n",
    "       pch=(1:length(output)),\n",
    "       col=(1:length(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Climate Risks to sites with dataARC\n",
    "\n",
    "Let's visualize how different concepts relate to different types of environments (albeit modern, in this example). Recall that first of the bioclimatic variables provided by WorldClim contains average annual temperature. Let's plot it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(clim_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a close look at the scale range. Those aren't degrees. They're decidegrees (i.e. 1/10 of one degree Celcius). Let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_p <- clim_p / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the average annual temperature of the farm and agriculture concepts, we use the \"extract\" function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output$Temp <- extract(clim_p,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can create histograms using the \"hist\" function from the FSA library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(output$Temp~output$CONCEPT, \n",
    "     same.ylim=FALSE, w=2.5,\n",
    "     xlab = \"Temperature (degrees Celcius)\",\n",
    "     ylab = \"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general perspective agrees with what we saw in the map. Sites labeled as church farms tend to be located in two distinct clusters on the island, and thus congregate around two temperature values. There're only a handful of tenant farms, so that histogram is heavily biased. Independent farms are biased towards the valleys of the north coast, also reflected in the histogram.\n",
    "\n",
    "One ever-growing danger to the preservation of sites is climate change, particularly with increasing temperatures. This largely affects the preservation of organic remains. Let's first create a map of how much the average annual temperature is expected to change by 2050\n",
    "\n",
    "Let's create a map of how much the average annual temperature is expected to change by 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_f <- clim_f / 10 # Don't forget to convert the predicted temperatures to degrees!\n",
    "change <- clim_f - clim_p\n",
    "plot(change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the climate change models aren't entirely finely-grained. They can really only predict broad trends in expected change. Regardless, let's get the temperature data for ALL sites, and identify the ones at greatest risk (we'll define it as more than 2 degrees Celsius of change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output <- arc[,lapply(.SD,mean), by=.(ID,CONCEPT), .SDcols = c(\"X\",\"Y\")][,.(X,Y,CONCEPT)] # Combine Sites\n",
    "output <- na.omit(output) # Exclude null values\n",
    "output <- SpatialPointsDataFrame(output[,.(X,Y)],output[,.(X,Y,CONCEPT)],proj4string=proj) # Convert to spatial\n",
    "output$TempChange <- extract(change,output) # Extract the temperature change\n",
    "warming_sites <- output[complete.cases(output@data$TempChange),] # Keep only sites with real values\n",
    "warming_sites <- warming_sites[warming_sites$TempChange >= 2.0,] # Only keep sites warming by more than 2 deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate sites at risk due to sea level rise (defined here as at an elevation at or below one meter above sea level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output$Elevation <- extract(dem,output) # Extract elevation\n",
    "flooding_sites <- output[complete.cases(output@data$Elevation),] # Keep only sites with real values\n",
    "flooding_sites <- flooding_sites[flooding_sites$Elevation <= 1,] # Only keep sites below 1 meter above sea level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can bring in the sites that the DataARC team has already, identified as at-risk using the *Environmental Threats to Icelandic Archaeological Sites* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets <- read.csv(\"DATASET_HASH_OUT.csv\",\n",
    "                     stringsAsFactors = F,\n",
    "                     encoding='UTF-8') # Get dataset hash list\n",
    "datasets <- as.data.table(datasets)\n",
    "datasets[, names(datasets) := lapply(.SD,trimws,which='left')]\n",
    "query <- \"Threat\" # Set our keyword\n",
    "threat <- datasets[str_detect(NAME,query),HASH] # Pull out the correct hash\n",
    "threat <- toupper(threat) # Since arc hashes are in all caps\n",
    "threatened_sites <- arc[DATASET == threat, \n",
    "                        lapply(.SD,mean), by=.(ID,CONCEPT),\n",
    "                        .SDcols = c(\"X\",\"Y\")][,.(X,Y,CONCEPT)]\n",
    "threatened_sites <- SpatialPointsDataFrame(threatened_sites[,.(X,Y)],\n",
    "                                          threatened_sites,\n",
    "                                          proj4string = proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize all of the at-risk sites together, we need to combine them into the same data.table object and convert it to a SpatialPointsDataFrame to plot with *mapview*. Alternatively, we can just plot with base each original file setting different display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = c(\"X\",\"Y\",\"CONCEPT\",\"TYPE\") # Define the columns we'll later keep\n",
    "warming_sites$TYPE <- \"Temperature\" # Assign risk types\n",
    "flooding_sites$TYPE <- \"Flooding\"\n",
    "threatened_sites$TYPE <- \"DataARC\"\n",
    "threatened <- data.table(rbind(warming_sites@data[,(cols)], # Combine risk types\n",
    "                               flooding_sites@data[,(cols)],\n",
    "                               threatened_sites@data[,(cols)]))\n",
    "threatened[, c(\"X\",\"Y\") := lapply(.SD,as.numeric),.SDcols = c(\"X\",\"Y\")]\n",
    "threatened_points <- SpatialPointsDataFrame(threatened[,.(X,Y)],threatened, proj4string = proj) # Convert to Spatial\n",
    "#mapview::mapview(threatened_points,zcol=\"TYPE\") # Plot\n",
    "\n",
    "plot(dem)\n",
    "plot(warming_sites, pch=1, col=1,add=T)\n",
    "plot(flooding_sites, pch=2, col=2,add=T)\n",
    "plot(threatened_sites, pch=3, col=3,add=T)\n",
    "legend(\"topright\", c(\"Temperature Change\",\"Flooding\",\"DataARC 'At-Risk'\"),\n",
    "       pch=1:3,col=1:3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore this data further, let's get a sense of what concepts are present throughout the threatened sites. We need to separate the concept strings into into rows as above, and swap out the hashes for concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threatened <- threatened[, .(CONCEPT = unlist(tstrsplit(CONCEPT,\":\"))), \n",
    "                         by=c(\"X\",\"Y\",\"TYPE\")]\n",
    "threatened[, CONCEPT := ..concepts$NAME[match(CONCEPT,..concepts$HASH)]]\n",
    "threatened <- na.omit(threatened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Quick aside: Why not just unpack all of the concepts in the beginning when we import DataArc for the first time? String operations can be very time-taking, even with data.table. When we consider that we'd have over 1,000,000 observations, some linked to as many as ten concepts, that's millions of potential operations. It's faster to winnow down the list to the sites we're interested in, and THEN split the columns.*\n",
    "\n",
    "Finally, we'll count the number of observations for each concept, and generate a wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts <- count(threatened$CONCEPT)\n",
    "wordcloud(counts$x,counts$freq,random.order = FALSE, colors=brewer.pal(8, \"Dark2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
